{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and SGD Homework \n",
    "***\n",
    "**Name**: Saikrishna Jaliparthy \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 9th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "In this homework you'll implement stochastic gradient ascent for logistic regression and you'll apply it to the task of determining whether documents are talking about automobiles or motorcycles.\n",
    "\n",
    "<br>\n",
    "\n",
    "![autos_motorcycles](autos_motorcycles.jpg \"A car and a motorcycle\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "You should not use any libraries that implement any of the functionality of logistic regression for this assignment; logistic regression is implemented in Scikit-Learn, but you should do everything by hand now. You'll be able to use library implementations of logistic regression in the future.\n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1: Loading and Exploring the Data\n",
    "***\n",
    "\n",
    "The `Example` class will be used to store the features and labels associated with a single training or test example.  The `read_data` function will read in the text data and split it into training and test sets.  \n",
    "\n",
    " Load the data and then do the following: \n",
    "- Report the number of words in the vocabulary \n",
    "- Explain how the code is creating features (i.e. what text model is being used). \n",
    "- Go into the raw text files in the data directory and figure out which label (0/1) refers to which class of document (automobiles or motorcycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kSEED = 1735\n",
    "kBIAS = \"BIAS_CONSTANT\"\n",
    "\n",
    "np.random.seed(kSEED)\n",
    "\n",
    "class Example:\n",
    "    \"\"\"\n",
    "    Class to represent a document example\n",
    "    \"\"\"\n",
    "    def __init__(self, label, words, vocab):\n",
    "        \"\"\"\n",
    "        Create a new example\n",
    "\n",
    "        :param label: The label (0 / 1) of the example\n",
    "        :param words: The words in a list of \"word:count\" format\n",
    "        :param vocab: The vocabulary to use as features (list)\n",
    "        \"\"\"\n",
    "        self.nonzero = {}\n",
    "        self.y = label\n",
    "        self.x = np.zeros(len(vocab))\n",
    "        for word, count in [x.split(\":\") for x in words]:\n",
    "            if word in vocab:\n",
    "                assert word != kBIAS, \"Bias can't actually appear in document\"\n",
    "                self.x[vocab.index(word)] += float(count)\n",
    "                self.nonzero[vocab.index(word)] = word\n",
    "        self.x[0] = 1\n",
    "\n",
    "def read_dataset(positive, negative, vocab, train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Reads in a text dataset with a given vocabulary\n",
    "\n",
    "    :param positive: Positive examples\n",
    "    :param negative: Negative examples\n",
    "    :param vocab: A list of vocabulary words\n",
    "    :param test_frac: How much of the data should be reserved for test\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = [x.split(\"\\t\")[0] for x in open(vocab, 'r') if '\\t' in x]\n",
    "    assert vocab[0] == kBIAS, \\\n",
    "        \"First vocab word must be bias term (was %s)\" % vocab[0]\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for label, input in [(1, positive), (0, negative)]:\n",
    "        for line in open(input):\n",
    "            ex = Example(label, line.split(), vocab)\n",
    "            if np.random.random() <= train_frac:\n",
    "                train_set.append(ex)\n",
    "            else:\n",
    "                test_set.append(ex)\n",
    "\n",
    "    # Shuffle the data \n",
    "    np.random.shuffle(train_set)\n",
    "    np.random.shuffle(test_set)\n",
    "\n",
    "    return train_set, test_set, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_fname = \"../data/autos_motorcycles/positive\"\n",
    "neg_fname = \"../data/autos_motorcycles/negative\"\n",
    "voc_fname = \"../data/autos_motorcycles/vocab\"\n",
    "train_set, test_set, vocab = read_dataset(pos_fname, neg_fname, voc_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of vocabulary is:  5327\n",
      "The text model being used is a Bag of words Model\n",
      "The Label for motorcycle is: 1 & The Label for Automobile is: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of vocabulary is: \",len(vocab))\n",
    "print(\"The text model being used is a Bag of words Model\")\n",
    "print(\"The Label for motorcycle is: 1 & The Label for Automobile is: 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Implementing SGD with Lazy Sparse Regularization\n",
    "***\n",
    "\n",
    "We've given you a class `LogReg` below which will train a logistic regression classifier to predict whether a document is talking about automobiles or motorcycles. \n",
    "\n",
    "**Part A**: In this problem you will modify the `sgd_update` function to perform **unregularized** stochastic gradient descent updates of the weights. Note that you should only update the weights for **non-zero** features, i.e. weights associated with words that appear in the current training example. The code below this cell demonstrates how to instantiate the class and train the classifier.   \n",
    "\n",
    "We've also given you unit tests in the next cell based on the simple example worked out in  the Lecture 4 in-class notebook.  At first your code will fail both of them. When your code is working you should pass tests called `test_unreg` and `test_learnrate`.  Do not move on to **Part A** until your code passes both of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    def __init__(self, train_set, test_set, lam, eta=0.1):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "\n",
    "        :param train_set: A set of training examples\n",
    "        :param test_set: A set of test examples \n",
    "        :param lam: Regularization parameter\n",
    "        :param eta: The learning rate to use \n",
    "        \"\"\"\n",
    "        \n",
    "        # Store training and test sets \n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set \n",
    "        \n",
    "        # Initialize vector of weights to zero  \n",
    "        self.w = np.zeros_like(train_set[0].x)\n",
    "        \n",
    "        # Store regularization parameter and eta function \n",
    "        self.lam = lam\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Create dictionary for lazy-sparse regularization\n",
    "        self.last_update = dict()\n",
    "\n",
    "        # Make sure regularization parameter is not negative \n",
    "        assert self.lam>= 0, \"Regularization parameter must be non-negative\"\n",
    "        \n",
    "        # Empty lists to store NLL and accuracy on train and test sets \n",
    "        self.train_nll = []\n",
    "        self.test_nll = []\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "    \n",
    "        \n",
    "    def sigmoid(self,score, threshold=20.0):\n",
    "        \"\"\"\n",
    "        Prevent overflow of exp by capping activation at 20.\n",
    "        You do not need to change this function. \n",
    "\n",
    "        :param score: A real valued number to convert into a number between 0 and 1\n",
    "        \"\"\"\n",
    "\n",
    "        # if score > threshold, cap value at score \n",
    "        if abs(score) > threshold:\n",
    "            score = threshold * np.sign(score)\n",
    "\n",
    "        return 1.0 / (1.0 + np.exp(-score)) \n",
    "\n",
    "    def compute_progress(self, examples):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the NLL and accuracy\n",
    "        You shouldn't need to change this function. \n",
    "\n",
    "        :param examples: The dataset to score\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        NLL = 0.0\n",
    "        num_correct = 0\n",
    "        for ex in examples:\n",
    "            # compute prob prediction\n",
    "            p = self.sigmoid(self.w.dot(ex.x))\n",
    "            # update negative log likelihood\n",
    "            NLL = NLL - np.log(p) if ex.y==1 else NLL - np.log(1.0-p)\n",
    "            # update number correct \n",
    "            num_correct += 1 if np.floor(p+.5)==ex.y else 0\n",
    "\n",
    "        return NLL, float(num_correct) / float(len(examples))\n",
    "    \n",
    "    def train(self, num_epochs=1, isVerbose=False, report_step=5):\n",
    "        \"\"\"\n",
    "        Train the logistic regression classifier on the training data \n",
    "\n",
    "        :param num_epochs: number of full passes over data to perform \n",
    "        :param isVerbose: boolean indicating whether to print progress\n",
    "        :param report_step: how many iterations between recording progress\n",
    "        \"\"\"\n",
    "        iteration = 0\n",
    "        # Perform an epoch \n",
    "        for pp in range(num_epochs):\n",
    "            # shuffle the data  \n",
    "            np.random.shuffle(self.train_set)\n",
    "            # loop over each training example\n",
    "            for ex in self.train_set:\n",
    "                # perform SGD update of weights \n",
    "                self.sgd_update(ex, iteration)\n",
    "                # record progress \n",
    "                if iteration % report_step == 1:\n",
    "                    train_nll, train_acc = self.compute_progress(self.train_set)\n",
    "                    test_nll, test_acc = self.compute_progress(self.test_set)\n",
    "                    self.train_nll.append(train_nll)\n",
    "                    self.test_nll.append(test_nll)\n",
    "                    self.train_acc.append(train_acc)\n",
    "                    self.test_acc.append(test_acc)\n",
    "                    if isVerbose:\n",
    "                        print(\"Update {: 5d}  TrnNLL {: 8.3f}  TstNLL {: 8.3f}  TrnA {:.3f}  TstA {:.3f}\"\n",
    "                             .format(iteration-1, train_nll, test_nll, train_acc, test_acc))\n",
    "                iteration += 1\n",
    "                \n",
    "    \n",
    "    def sgd_update(self, train_example, iteration):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the NLL \n",
    "\n",
    "        :param train_example: The example to take the gradient with respect to\n",
    "        :param iteration: The current iteration (an integer)\n",
    "        \"\"\"\n",
    "        # TODO implement LSR updates of weights \n",
    "        self.w = self.w\n",
    "        \n",
    "        x=train_example.x\n",
    "        y=train_example.y\n",
    "        \n",
    "        nz=train_example.nonzero\n",
    "        sig=self.sigmoid(self.w.dot(x))\n",
    "        for i in nz:\n",
    "            if iteration==0: \n",
    "                self.last_update[i]=0\n",
    "            if i not in self.last_update:\n",
    "                self.last_update[i]=0\n",
    "            self.w[i]=self.w[i]-(self.eta*(sig-y)*x[i])\n",
    "            self.w[i]=self.w[i]*np.power((1-2*self.eta*self.lam),(iteration-self.last_update[i]+1))\n",
    "            self.last_update[i]=iteration+1\n",
    "        self.w[0]=self.w[0]-(self.eta*(sig-y))\n",
    "        return self.w\n",
    "    \n",
    "\n",
    "    \n",
    "    def bestmowords(self):\n",
    "        z=[]\n",
    "        zc=np.array(self.w)\n",
    "        for i in range(0,10):\n",
    "            wx= zc==np.amax(zc)\n",
    "            idx=np.where(wx)[0]\n",
    "            #print(idx)\n",
    "            z.append(idx[0])\n",
    "            zc[idx]=0\n",
    "            i=i+1\n",
    "        return z\n",
    "        \n",
    "        \n",
    "    def bestauwords(self):\n",
    "        z1=[]\n",
    "        zc1=np.array(self.w)\n",
    "        for i1 in range(0,10):\n",
    "            idx1=np.where(zc1==np.amin(zc1))[0]\n",
    "            #print(idx)\n",
    "            z1.append(idx1[0])\n",
    "            zc1[idx1]=0\n",
    "            i1=i1+1\n",
    "        return z1\n",
    "    \n",
    "    def badwords(self):\n",
    "        z2=[]\n",
    "        qwe=np.argsort((self.beta))\n",
    "        middle=(len(qwe)-1)/2\n",
    "        for i2 in range(middle-5,middle):\n",
    "            z2.append(i2)\n",
    "        for i3 in range(middle,middle+5):\n",
    "            z2.append(i3)\n",
    "        \n",
    "        return z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update     0  TrnNLL  700.586  TstNLL   79.653  TrnA 0.608  TstA 0.586\n",
      "Update     5  TrnNLL  645.451  TstNLL   75.838  TrnA 0.655  TstA 0.612\n",
      "Update    10  TrnNLL  668.710  TstNLL   77.493  TrnA 0.615  TstA 0.595\n",
      "Update    15  TrnNLL  591.619  TstNLL   68.293  TrnA 0.696  TstA 0.690\n",
      "Update    20  TrnNLL  515.695  TstNLL   61.158  TrnA 0.777  TstA 0.733\n",
      "Update    25  TrnNLL  481.967  TstNLL   57.221  TrnA 0.800  TstA 0.836\n",
      "Update    30  TrnNLL  458.205  TstNLL   54.202  TrnA 0.797  TstA 0.819\n",
      "Update    35  TrnNLL  435.343  TstNLL   51.553  TrnA 0.817  TstA 0.784\n",
      "Update    40  TrnNLL  406.357  TstNLL   47.607  TrnA 0.853  TstA 0.819\n",
      "Update    45  TrnNLL  382.208  TstNLL   43.775  TrnA 0.854  TstA 0.871\n",
      "Update    50  TrnNLL  378.130  TstNLL   43.830  TrnA 0.862  TstA 0.845\n",
      "Update    55  TrnNLL  358.611  TstNLL   43.266  TrnA 0.860  TstA 0.862\n",
      "Update    60  TrnNLL  350.285  TstNLL   42.177  TrnA 0.879  TstA 0.853\n",
      "Update    65  TrnNLL  338.611  TstNLL   41.414  TrnA 0.881  TstA 0.853\n",
      "Update    70  TrnNLL  335.487  TstNLL   42.093  TrnA 0.868  TstA 0.862\n",
      "Update    75  TrnNLL  330.448  TstNLL   42.075  TrnA 0.868  TstA 0.871\n",
      "Update    80  TrnNLL  319.162  TstNLL   38.808  TrnA 0.887  TstA 0.853\n",
      "Update    85  TrnNLL  316.586  TstNLL   36.819  TrnA 0.889  TstA 0.905\n",
      "Update    90  TrnNLL  307.541  TstNLL   35.497  TrnA 0.889  TstA 0.897\n",
      "Update    95  TrnNLL  298.688  TstNLL   34.924  TrnA 0.893  TstA 0.888\n",
      "Update   100  TrnNLL  297.631  TstNLL   34.684  TrnA 0.894  TstA 0.888\n",
      "Update   105  TrnNLL  290.185  TstNLL   33.137  TrnA 0.901  TstA 0.914\n",
      "Update   110  TrnNLL  289.614  TstNLL   33.299  TrnA 0.901  TstA 0.897\n",
      "Update   115  TrnNLL  287.129  TstNLL   34.139  TrnA 0.899  TstA 0.888\n",
      "Update   120  TrnNLL  316.505  TstNLL   37.032  TrnA 0.864  TstA 0.871\n",
      "Update   125  TrnNLL  273.540  TstNLL   31.541  TrnA 0.903  TstA 0.922\n",
      "Update   130  TrnNLL  276.297  TstNLL   33.045  TrnA 0.902  TstA 0.897\n",
      "Update   135  TrnNLL  273.605  TstNLL   33.689  TrnA 0.900  TstA 0.905\n",
      "Update   140  TrnNLL  261.794  TstNLL   31.171  TrnA 0.901  TstA 0.879\n",
      "Update   145  TrnNLL  261.383  TstNLL   30.201  TrnA 0.900  TstA 0.888\n",
      "Update   150  TrnNLL  257.694  TstNLL   29.623  TrnA 0.901  TstA 0.897\n",
      "Update   155  TrnNLL  254.828  TstNLL   29.541  TrnA 0.905  TstA 0.897\n",
      "Update   160  TrnNLL  255.537  TstNLL   29.830  TrnA 0.900  TstA 0.897\n",
      "Update   165  TrnNLL  358.330  TstNLL   52.402  TrnA 0.865  TstA 0.853\n",
      "Update   170  TrnNLL  315.548  TstNLL   46.173  TrnA 0.883  TstA 0.862\n",
      "Update   175  TrnNLL  306.735  TstNLL   45.123  TrnA 0.886  TstA 0.862\n",
      "Update   180  TrnNLL  292.073  TstNLL   43.432  TrnA 0.894  TstA 0.879\n",
      "Update   185  TrnNLL  254.944  TstNLL   33.779  TrnA 0.904  TstA 0.879\n",
      "Update   190  TrnNLL  252.006  TstNLL   32.948  TrnA 0.905  TstA 0.888\n",
      "Update   195  TrnNLL  247.979  TstNLL   35.737  TrnA 0.904  TstA 0.905\n",
      "Update   200  TrnNLL  244.863  TstNLL   35.351  TrnA 0.907  TstA 0.905\n",
      "Update   205  TrnNLL  247.229  TstNLL   35.478  TrnA 0.900  TstA 0.914\n",
      "Update   210  TrnNLL  240.150  TstNLL   34.264  TrnA 0.905  TstA 0.914\n",
      "Update   215  TrnNLL  241.396  TstNLL   28.826  TrnA 0.904  TstA 0.922\n",
      "Update   220  TrnNLL  238.054  TstNLL   28.638  TrnA 0.913  TstA 0.914\n",
      "Update   225  TrnNLL  240.060  TstNLL   29.795  TrnA 0.907  TstA 0.897\n",
      "Update   230  TrnNLL  227.982  TstNLL   28.238  TrnA 0.919  TstA 0.922\n",
      "Update   235  TrnNLL  227.604  TstNLL   28.259  TrnA 0.916  TstA 0.922\n",
      "Update   240  TrnNLL  225.853  TstNLL   27.690  TrnA 0.918  TstA 0.922\n",
      "Update   245  TrnNLL  225.139  TstNLL   27.629  TrnA 0.918  TstA 0.914\n",
      "Update   250  TrnNLL  213.482  TstNLL   29.181  TrnA 0.929  TstA 0.905\n",
      "Update   255  TrnNLL  211.885  TstNLL   29.055  TrnA 0.929  TstA 0.905\n",
      "Update   260  TrnNLL  211.757  TstNLL   27.590  TrnA 0.925  TstA 0.905\n",
      "Update   265  TrnNLL  208.949  TstNLL   27.635  TrnA 0.925  TstA 0.897\n",
      "Update   270  TrnNLL  211.366  TstNLL   27.508  TrnA 0.926  TstA 0.905\n",
      "Update   275  TrnNLL  210.288  TstNLL   27.359  TrnA 0.927  TstA 0.905\n",
      "Update   280  TrnNLL  214.745  TstNLL   27.120  TrnA 0.923  TstA 0.905\n",
      "Update   285  TrnNLL  204.837  TstNLL   26.499  TrnA 0.927  TstA 0.922\n",
      "Update   290  TrnNLL  207.498  TstNLL   26.276  TrnA 0.927  TstA 0.914\n",
      "Update   295  TrnNLL  221.439  TstNLL   26.817  TrnA 0.919  TstA 0.905\n",
      "Update   300  TrnNLL  207.371  TstNLL   25.455  TrnA 0.926  TstA 0.914\n",
      "Update   305  TrnNLL  188.437  TstNLL   25.352  TrnA 0.930  TstA 0.914\n",
      "Update   310  TrnNLL  189.036  TstNLL   24.922  TrnA 0.933  TstA 0.931\n",
      "Update   315  TrnNLL  186.841  TstNLL   25.348  TrnA 0.936  TstA 0.922\n",
      "Update   320  TrnNLL  185.405  TstNLL   24.802  TrnA 0.934  TstA 0.940\n",
      "Update   325  TrnNLL  182.927  TstNLL   24.943  TrnA 0.936  TstA 0.922\n",
      "Update   330  TrnNLL  182.521  TstNLL   24.919  TrnA 0.936  TstA 0.940\n",
      "Update   335  TrnNLL  182.040  TstNLL   24.422  TrnA 0.934  TstA 0.931\n",
      "Update   340  TrnNLL  182.306  TstNLL   24.230  TrnA 0.932  TstA 0.931\n",
      "Update   345  TrnNLL  182.871  TstNLL   24.408  TrnA 0.934  TstA 0.922\n",
      "Update   350  TrnNLL  182.566  TstNLL   24.440  TrnA 0.932  TstA 0.922\n",
      "Update   355  TrnNLL  181.825  TstNLL   24.218  TrnA 0.934  TstA 0.922\n",
      "Update   360  TrnNLL  179.963  TstNLL   23.847  TrnA 0.936  TstA 0.922\n",
      "Update   365  TrnNLL  183.479  TstNLL   23.843  TrnA 0.931  TstA 0.914\n",
      "Update   370  TrnNLL  171.698  TstNLL   22.211  TrnA 0.936  TstA 0.922\n",
      "Update   375  TrnNLL  167.477  TstNLL   22.550  TrnA 0.938  TstA 0.931\n",
      "Update   380  TrnNLL  178.350  TstNLL   20.352  TrnA 0.935  TstA 0.922\n",
      "Update   385  TrnNLL  171.579  TstNLL   19.536  TrnA 0.937  TstA 0.940\n",
      "Update   390  TrnNLL  169.543  TstNLL   19.416  TrnA 0.937  TstA 0.957\n",
      "Update   395  TrnNLL  172.012  TstNLL   19.643  TrnA 0.937  TstA 0.922\n",
      "Update   400  TrnNLL  180.456  TstNLL   20.631  TrnA 0.932  TstA 0.922\n",
      "Update   405  TrnNLL  183.901  TstNLL   21.115  TrnA 0.930  TstA 0.922\n",
      "Update   410  TrnNLL  169.584  TstNLL   22.763  TrnA 0.937  TstA 0.931\n",
      "Update   415  TrnNLL  181.086  TstNLL   26.207  TrnA 0.936  TstA 0.914\n",
      "Update   420  TrnNLL  172.304  TstNLL   24.339  TrnA 0.935  TstA 0.922\n",
      "Update   425  TrnNLL  178.975  TstNLL   26.243  TrnA 0.933  TstA 0.905\n",
      "Update   430  TrnNLL  178.872  TstNLL   26.436  TrnA 0.933  TstA 0.905\n",
      "Update   435  TrnNLL  179.088  TstNLL   26.718  TrnA 0.935  TstA 0.914\n",
      "Update   440  TrnNLL  183.330  TstNLL   27.742  TrnA 0.935  TstA 0.914\n",
      "Update   445  TrnNLL  180.944  TstNLL   26.026  TrnA 0.936  TstA 0.922\n",
      "Update   450  TrnNLL  187.053  TstNLL   27.729  TrnA 0.934  TstA 0.922\n",
      "Update   455  TrnNLL  166.207  TstNLL   23.681  TrnA 0.942  TstA 0.922\n",
      "Update   460  TrnNLL  158.722  TstNLL   22.205  TrnA 0.941  TstA 0.940\n",
      "Update   465  TrnNLL  163.151  TstNLL   23.532  TrnA 0.942  TstA 0.922\n",
      "Update   470  TrnNLL  154.578  TstNLL   22.000  TrnA 0.941  TstA 0.940\n",
      "Update   475  TrnNLL  163.007  TstNLL   25.987  TrnA 0.942  TstA 0.914\n",
      "Update   480  TrnNLL  166.112  TstNLL   26.811  TrnA 0.941  TstA 0.914\n",
      "Update   485  TrnNLL  168.218  TstNLL   27.350  TrnA 0.944  TstA 0.914\n",
      "Update   490  TrnNLL  139.711  TstNLL   21.431  TrnA 0.951  TstA 0.948\n",
      "Update   495  TrnNLL  138.345  TstNLL   21.030  TrnA 0.952  TstA 0.948\n",
      "Update   500  TrnNLL  137.156  TstNLL   20.805  TrnA 0.953  TstA 0.948\n",
      "Update   505  TrnNLL  132.418  TstNLL   19.741  TrnA 0.953  TstA 0.940\n",
      "Update   510  TrnNLL  131.417  TstNLL   19.308  TrnA 0.955  TstA 0.940\n",
      "Update   515  TrnNLL  131.057  TstNLL   18.822  TrnA 0.956  TstA 0.948\n",
      "Update   520  TrnNLL  127.231  TstNLL   19.013  TrnA 0.958  TstA 0.948\n",
      "Update   525  TrnNLL  126.730  TstNLL   19.275  TrnA 0.959  TstA 0.948\n",
      "Update   530  TrnNLL  125.831  TstNLL   19.393  TrnA 0.958  TstA 0.940\n",
      "Update   535  TrnNLL  126.684  TstNLL   19.032  TrnA 0.957  TstA 0.940\n",
      "Update   540  TrnNLL  125.542  TstNLL   18.851  TrnA 0.961  TstA 0.948\n",
      "Update   545  TrnNLL  125.146  TstNLL   18.831  TrnA 0.963  TstA 0.940\n",
      "Update   550  TrnNLL  123.315  TstNLL   18.580  TrnA 0.961  TstA 0.940\n",
      "Update   555  TrnNLL  123.189  TstNLL   18.422  TrnA 0.960  TstA 0.940\n",
      "Update   560  TrnNLL  128.249  TstNLL   22.092  TrnA 0.958  TstA 0.931\n",
      "Update   565  TrnNLL  121.390  TstNLL   19.729  TrnA 0.960  TstA 0.940\n",
      "Update   570  TrnNLL  121.550  TstNLL   19.808  TrnA 0.960  TstA 0.940\n",
      "Update   575  TrnNLL  124.409  TstNLL   21.415  TrnA 0.962  TstA 0.931\n",
      "Update   580  TrnNLL  130.248  TstNLL   23.135  TrnA 0.957  TstA 0.931\n",
      "Update   585  TrnNLL  120.684  TstNLL   19.227  TrnA 0.961  TstA 0.940\n",
      "Update   590  TrnNLL  121.333  TstNLL   19.534  TrnA 0.963  TstA 0.940\n",
      "Update   595  TrnNLL  121.593  TstNLL   18.522  TrnA 0.962  TstA 0.948\n",
      "Update   600  TrnNLL  124.607  TstNLL   20.985  TrnA 0.964  TstA 0.931\n",
      "Update   605  TrnNLL  118.914  TstNLL   19.801  TrnA 0.968  TstA 0.940\n",
      "Update   610  TrnNLL  117.674  TstNLL   19.421  TrnA 0.967  TstA 0.940\n",
      "Update   615  TrnNLL  116.515  TstNLL   18.483  TrnA 0.964  TstA 0.948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update   620  TrnNLL  117.846  TstNLL   17.245  TrnA 0.958  TstA 0.948\n",
      "Update   625  TrnNLL  116.958  TstNLL   17.205  TrnA 0.961  TstA 0.948\n",
      "Update   630  TrnNLL  115.375  TstNLL   16.196  TrnA 0.959  TstA 0.966\n",
      "Update   635  TrnNLL  112.811  TstNLL   18.034  TrnA 0.967  TstA 0.948\n",
      "Update   640  TrnNLL  110.915  TstNLL   18.168  TrnA 0.966  TstA 0.957\n",
      "Update   645  TrnNLL  110.372  TstNLL   18.565  TrnA 0.968  TstA 0.948\n",
      "Update   650  TrnNLL  109.409  TstNLL   17.897  TrnA 0.967  TstA 0.957\n",
      "Update   655  TrnNLL  109.240  TstNLL   17.586  TrnA 0.966  TstA 0.957\n",
      "Update   660  TrnNLL  109.284  TstNLL   17.549  TrnA 0.967  TstA 0.957\n",
      "Update   665  TrnNLL  108.748  TstNLL   17.229  TrnA 0.965  TstA 0.966\n",
      "Update   670  TrnNLL  112.170  TstNLL   17.608  TrnA 0.965  TstA 0.966\n",
      "Update   675  TrnNLL  108.031  TstNLL   17.314  TrnA 0.967  TstA 0.966\n",
      "Update   680  TrnNLL  107.184  TstNLL   18.762  TrnA 0.972  TstA 0.948\n",
      "Update   685  TrnNLL  106.946  TstNLL   18.759  TrnA 0.972  TstA 0.948\n",
      "Update   690  TrnNLL  105.535  TstNLL   18.020  TrnA 0.970  TstA 0.948\n",
      "Update   695  TrnNLL  105.570  TstNLL   18.062  TrnA 0.970  TstA 0.948\n",
      "Update   700  TrnNLL  105.288  TstNLL   17.660  TrnA 0.967  TstA 0.957\n",
      "Update   705  TrnNLL  104.999  TstNLL   17.756  TrnA 0.969  TstA 0.948\n",
      "Update   710  TrnNLL  104.617  TstNLL   18.942  TrnA 0.970  TstA 0.948\n",
      "Update   715  TrnNLL  103.723  TstNLL   18.684  TrnA 0.971  TstA 0.948\n",
      "Update   720  TrnNLL  102.801  TstNLL   17.912  TrnA 0.973  TstA 0.948\n",
      "Update   725  TrnNLL  102.879  TstNLL   17.940  TrnA 0.973  TstA 0.948\n",
      "Update   730  TrnNLL  102.660  TstNLL   18.005  TrnA 0.973  TstA 0.948\n",
      "Update   735  TrnNLL  102.620  TstNLL   17.987  TrnA 0.973  TstA 0.948\n",
      "Update   740  TrnNLL  103.032  TstNLL   19.145  TrnA 0.971  TstA 0.948\n",
      "Update   745  TrnNLL  101.323  TstNLL   18.364  TrnA 0.975  TstA 0.948\n",
      "Update   750  TrnNLL  100.399  TstNLL   17.932  TrnA 0.975  TstA 0.957\n",
      "Update   755  TrnNLL   99.200  TstNLL   18.427  TrnA 0.974  TstA 0.948\n",
      "Update   760  TrnNLL   98.311  TstNLL   16.953  TrnA 0.975  TstA 0.957\n",
      "Update   765  TrnNLL   98.031  TstNLL   17.016  TrnA 0.976  TstA 0.957\n",
      "Update   770  TrnNLL   96.559  TstNLL   17.136  TrnA 0.977  TstA 0.948\n",
      "Update   775  TrnNLL   96.480  TstNLL   17.171  TrnA 0.977  TstA 0.948\n",
      "Update   780  TrnNLL   96.660  TstNLL   16.266  TrnA 0.977  TstA 0.957\n",
      "Update   785  TrnNLL   96.658  TstNLL   16.045  TrnA 0.976  TstA 0.957\n",
      "Update   790  TrnNLL   94.383  TstNLL   16.305  TrnA 0.979  TstA 0.957\n",
      "Update   795  TrnNLL   96.617  TstNLL   15.825  TrnA 0.975  TstA 0.957\n",
      "Update   800  TrnNLL  101.057  TstNLL   15.872  TrnA 0.973  TstA 0.957\n",
      "Update   805  TrnNLL   97.006  TstNLL   16.080  TrnA 0.977  TstA 0.957\n",
      "Update   810  TrnNLL   92.080  TstNLL   15.956  TrnA 0.979  TstA 0.957\n",
      "Update   815  TrnNLL   92.054  TstNLL   15.958  TrnA 0.979  TstA 0.957\n",
      "Update   820  TrnNLL   93.092  TstNLL   15.856  TrnA 0.977  TstA 0.957\n",
      "Update   825  TrnNLL   91.092  TstNLL   15.839  TrnA 0.979  TstA 0.957\n",
      "Update   830  TrnNLL   88.750  TstNLL   16.138  TrnA 0.980  TstA 0.957\n",
      "Update   835  TrnNLL   85.194  TstNLL   16.723  TrnA 0.980  TstA 0.957\n",
      "Update   840  TrnNLL   85.178  TstNLL   16.737  TrnA 0.980  TstA 0.957\n",
      "Update   845  TrnNLL   85.113  TstNLL   16.370  TrnA 0.980  TstA 0.957\n",
      "Update   850  TrnNLL   85.394  TstNLL   16.087  TrnA 0.980  TstA 0.957\n",
      "Update   855  TrnNLL   85.236  TstNLL   16.498  TrnA 0.980  TstA 0.957\n",
      "Update   860  TrnNLL   87.201  TstNLL   15.956  TrnA 0.979  TstA 0.957\n",
      "Update   865  TrnNLL   85.972  TstNLL   15.934  TrnA 0.978  TstA 0.957\n",
      "Update   870  TrnNLL   84.179  TstNLL   19.196  TrnA 0.982  TstA 0.940\n",
      "Update   875  TrnNLL   84.377  TstNLL   19.423  TrnA 0.983  TstA 0.940\n",
      "Update   880  TrnNLL   92.990  TstNLL   14.388  TrnA 0.972  TstA 0.957\n",
      "Update   885  TrnNLL   87.962  TstNLL   14.251  TrnA 0.976  TstA 0.948\n",
      "Update   890  TrnNLL   91.931  TstNLL   14.844  TrnA 0.973  TstA 0.957\n",
      "Update   895  TrnNLL   92.792  TstNLL   14.927  TrnA 0.973  TstA 0.957\n",
      "Update   900  TrnNLL   92.084  TstNLL   14.840  TrnA 0.973  TstA 0.957\n",
      "Update   905  TrnNLL   89.768  TstNLL   14.565  TrnA 0.974  TstA 0.948\n",
      "Update   910  TrnNLL   86.499  TstNLL   14.461  TrnA 0.975  TstA 0.948\n",
      "Update   915  TrnNLL   85.684  TstNLL   14.363  TrnA 0.976  TstA 0.948\n",
      "Update   920  TrnNLL   85.630  TstNLL   14.484  TrnA 0.976  TstA 0.948\n",
      "Update   925  TrnNLL   90.321  TstNLL   14.647  TrnA 0.973  TstA 0.957\n",
      "Update   930  TrnNLL   97.164  TstNLL   16.111  TrnA 0.970  TstA 0.957\n",
      "Update   935  TrnNLL   92.186  TstNLL   15.518  TrnA 0.972  TstA 0.957\n",
      "Update   940  TrnNLL   70.593  TstNLL   13.053  TrnA 0.986  TstA 0.974\n",
      "Update   945  TrnNLL  102.053  TstNLL   23.721  TrnA 0.967  TstA 0.931\n",
      "Update   950  TrnNLL  100.793  TstNLL   23.562  TrnA 0.968  TstA 0.931\n",
      "Update   955  TrnNLL   92.708  TstNLL   21.786  TrnA 0.973  TstA 0.940\n",
      "Update   960  TrnNLL   82.916  TstNLL   20.237  TrnA 0.979  TstA 0.948\n",
      "Update   965  TrnNLL   84.239  TstNLL   20.555  TrnA 0.978  TstA 0.948\n",
      "Update   970  TrnNLL   87.694  TstNLL   21.051  TrnA 0.974  TstA 0.948\n",
      "Update   975  TrnNLL   88.409  TstNLL   21.205  TrnA 0.974  TstA 0.940\n",
      "Update   980  TrnNLL   84.842  TstNLL   20.653  TrnA 0.977  TstA 0.948\n",
      "Update   985  TrnNLL   78.919  TstNLL   19.152  TrnA 0.980  TstA 0.948\n",
      "Update   990  TrnNLL   69.205  TstNLL   17.129  TrnA 0.986  TstA 0.940\n",
      "Update   995  TrnNLL   65.125  TstNLL   17.036  TrnA 0.982  TstA 0.940\n",
      "Update  1000  TrnNLL   64.387  TstNLL   17.911  TrnA 0.982  TstA 0.922\n",
      "Update  1005  TrnNLL   64.195  TstNLL   17.867  TrnA 0.983  TstA 0.922\n",
      "Update  1010  TrnNLL   67.182  TstNLL   18.552  TrnA 0.980  TstA 0.922\n",
      "Update  1015  TrnNLL   65.010  TstNLL   18.354  TrnA 0.983  TstA 0.905\n",
      "Update  1020  TrnNLL   65.475  TstNLL   18.133  TrnA 0.982  TstA 0.922\n",
      "Update  1025  TrnNLL   65.202  TstNLL   17.937  TrnA 0.982  TstA 0.922\n",
      "Update  1030  TrnNLL   63.874  TstNLL   17.778  TrnA 0.983  TstA 0.931\n",
      "Update  1035  TrnNLL   60.691  TstNLL   17.060  TrnA 0.985  TstA 0.931\n",
      "Update  1040  TrnNLL   60.909  TstNLL   16.933  TrnA 0.983  TstA 0.931\n",
      "Update  1045  TrnNLL   61.826  TstNLL   17.340  TrnA 0.985  TstA 0.922\n",
      "Update  1050  TrnNLL   62.774  TstNLL   17.904  TrnA 0.985  TstA 0.914\n",
      "Update  1055  TrnNLL   63.636  TstNLL   18.163  TrnA 0.985  TstA 0.914\n",
      "Update  1060  TrnNLL   61.136  TstNLL   17.772  TrnA 0.984  TstA 0.931\n",
      "Update  1065  TrnNLL   61.178  TstNLL   17.779  TrnA 0.984  TstA 0.931\n",
      "Update  1070  TrnNLL   52.392  TstNLL   16.403  TrnA 0.989  TstA 0.931\n"
     ]
    }
   ],
   "source": [
    "lr = LogReg(train_set, test_set, lam=0, eta=0.1)\n",
    "lr.train(isVerbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit tests are located in the script `tests.py` in this directory.  Execute the following cell to call the script and run the tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_unreg (__main__.TestLogReg) ... ok\n",
      "test_learnrate (__main__.TestLogReg) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5  2.   1.5  0.5  0. ]\n",
      "[-0.49330715  2.         -0.4866143  -2.47992145 -0.99330715]\n",
      "[ 0.25  1.    0.75  0.25  0.  ]\n",
      "[-0.21207091  1.         -0.17414182 -1.13621273 -0.46207091]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.009s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your unregularized updates are working, modify the `sgd_update` function again to perform regularized updates using **Lazy Sparse Regularization**. Note that you should not regularize the bias weight. See the Lecture 4 in-class notebook for a refresher on LSR. **Note**: After implementing LSR, your code should still pass the unit tests for **Part A** when `lam = 0`. \n",
    "\n",
    "We've given you a third unit test in the next cell called `test_reg` based on the simple example of LSR worked out in  the Lecture 4 in-class notebook.  Do not move on to **Problem 3** until your code passes the test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_reg (__main__.TestLogReg) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5   1.    0.75  0.25  0.  ]\n",
      "[-0.43991335  1.         -0.56491335 -1.28487002 -0.23497834]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 3: Hyperparameter Tuning \n",
    "***\n",
    "\n",
    "**Part A**: Perform a systematic study of the effect of the regularization parameter on the accuracy of your classifier on the test set.  Which choice of `lam` seems to do the best?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10c31b390>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5P8Drz730aPpvDcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bfda198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=[]\n",
    "v=np.linspace(0,.01,10)\n",
    "for i in v:\n",
    "    lr = LogReg(train_set, test_set, lam=i, eta=0.11)\n",
    "    lr.train()\n",
    "    o.append(lr.test_acc[-1])\n",
    "    #print(\"The regularization parameter is: \",i, \"Accuracy is: \",lr.test_acc[-1])\n",
    "plt.plot(v,o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above illustration of the graph between regularization parameter and the accuracy of the classifier. We can say that for different values of regularization parameter we will arrive at different accuracy for the classifieir. So from above graph we can conclude the maximum accuracy of the classifier is achieved at regularization parameter 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: For the value of `lam` chosen in **Part A** perform a systematic study of the choice of learning rate on the speed of convergence SGD.  Which learning rate seems to give the fastest convergence?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/studies/machinelearning/CSCI5622-Machine-Learning/hmwk/logreg/tests.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mtrain_nl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mtest_nl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mtrain_nl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_nl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mtest_nl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_nl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mtrain_ac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "lra=[]\n",
    "it=[]\n",
    "iteration=0\n",
    "report_step=200\n",
    "num_epochs=5\n",
    "v=np.linspace(0.01,.05,5)\n",
    "for i in v:\n",
    "    lg = LogReg(train_set, test_set, lam=0.001, eta=i)\n",
    "    #lra.append(i)\n",
    "    for pp in range(num_epochs):\n",
    "        np.random.shuffle(train_set)\n",
    "        for ex in train_set:\n",
    "            lg.sgd_update(ex, iteration)\n",
    "            if iteration % report_step == 1:\n",
    "                train_nl, train_ac = self.compute_progress(self.train_set)\n",
    "                test_nl, test_ac = self.compute_progress(self.test_set)\n",
    "                train_nl.append(train_nl)\n",
    "                test_nl.append(test_nl)\n",
    "                train_ac.append(train_ac)\n",
    "                self.test_acc.append(test_acc)\n",
    "                lra.append(train_nl)\n",
    "                it.append(iteration-1)\n",
    "            iteration += 1\n",
    "#print(it)\n",
    "#print(v)\n",
    "#plt.plot(it,lra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate has an effect on speed of convergence of SGD. If the learning rate is  too small it will take a way long to optimal solution and if the learning rate is too large will miss the optimal solution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 4: Identifying Predictive and Non-Predictive Words \n",
    "***\n",
    "\n",
    "**Part A**: Find the top 10 words that are the best predictors for each class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motorcycle: \n",
      "['dod', 'bike', 'mm', 'ca', 'bikes', 'rider', 'ride', 'riding', 'shaft', 'sun']\n",
      "Automobile: \n",
      "['road', 'car', 'course', 'cars', 'four', 'wheel', 'participants', 'points', 'BIAS_CONSTANT', 'set']\n"
     ]
    }
   ],
   "source": [
    "lr = LogReg(train_set, test_set, lam=0.01, eta=0.1)\n",
    "lr.train()\n",
    "om=[]\n",
    "print(\"Motorcycle: \")\n",
    "mo=lr.bestmowords()\n",
    "for i in mo:\n",
    "    om.append(vocab[i])\n",
    "print(om)\n",
    "\n",
    "oa=[]\n",
    "print(\"Automobile: \")\n",
    "au=lr.bestauwords()\n",
    "for i in au:\n",
    "    oa.append(vocab[i])\n",
    "print(oa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Find the 10 words that are the worst predictors for class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'argsort' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/studies/machinelearning/CSCI5622-Machine-Learning/hmwk/logreg/tests.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Worst: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbadwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studies/machinelearning/CSCI5622-Machine-Learning/hmwk/logreg/tests.py\u001b[0m in \u001b[0;36mbadwords\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbadwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mz2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mqwe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mmiddle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqwe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiddle\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmiddle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argsort' is not defined"
     ]
    }
   ],
   "source": [
    "lr = LogReg(train_set, test_set, lam=0.01, eta=0.1)\n",
    "lr.train()\n",
    "ow=[]\n",
    "print(\"Worst: \")\n",
    "bad = lr.badwords()\n",
    "for i in bad:\n",
    "    ow.append(vocab[i])\n",
    "print(ow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
