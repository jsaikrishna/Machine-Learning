{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Homework \n",
    "***\n",
    "**Name**: Saikrishna Jaliparthy\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday April 13th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "In this homework you'll implement the AdaBoost classification framework to do handwritten digit recognition. Your implementation should be based on the description of AdaBoost given in the lecture slides.\n",
    "\n",
    "<br>\n",
    "\n",
    "![digits](mnist.png \"mnist data\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** use sklearn's implementation of Adaboost.  You may however use sklearn's implementation of decisions trees. \n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:16.624747Z",
     "start_time": "2018-04-02T09:58:15.916585Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone \n",
    "import matplotlib.pylab as plt\n",
    "import math\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1\n",
    "***\n",
    "\n",
    "Since we'll be working with binary classifiers, we'll look at the subset of the MNIST data pertaining to handwritten three's and eights. Note that we'll also be using a lower-res version of the MNIST data used in the KNN homework. The class below will load, parse, and store the subset of the. Load the data and then report: \n",
    "\n",
    "- The number of examples in the training set \n",
    "- The number of examples in the validation set \n",
    "- The number of pixels in each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:18.840016Z",
     "start_time": "2018-04-02T09:58:18.805889Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThreesAndEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set \n",
    "        X_train, y_train, X_valid, y_valid = pickle.load(f)\n",
    "\n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.X_train = X_train[np.logical_or( y_train==3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or( y_train==3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.X_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.X_train = self.X_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.X_valid = X_valid[np.logical_or( y_valid==3, y_valid == 8), :]\n",
    "        self.y_valid = y_valid[np.logical_or( y_valid==3, y_valid == 8)]\n",
    "        self.y_valid = np.array([1 if y == 8 else -1 for y in self.y_valid])\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:19.740678Z",
     "start_time": "2018-04-02T09:58:19.609721Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ThreesAndEights(\"../data/mnist21x21_3789.pklz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples in the training set: 2000\n",
      "The number of examples in the validation set: 500\n",
      "The number of pixels in each image 441\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of examples in the training set:\",len(data.X_train))\n",
    "print(\"The number of examples in the validation set:\",len(data.X_valid))\n",
    "print(\"The number of pixels in each image\",len(data.X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2: Implementing AdaBoost  \n",
    "***\n",
    "\n",
    "We've given you a skeleton of the class `AdaBoost` below which will train a classifier based on boosted shallow decision trees as implemented by sklearn. Take a look at the class skeleton first so that you understand the underlying organization and data structures that we'll be using.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:23.101470Z",
     "start_time": "2018-04-02T09:58:23.016833Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_learners=20, base=DecisionTreeClassifier(max_depth=1), random_state=1234):\n",
    "        \"\"\"\n",
    "        Create a new adaboost classifier.\n",
    "        \n",
    "        Args:\n",
    "            N (int, optional): Number of weak learners in classifier.\n",
    "            base (BaseEstimator, optional): Your general weak learner \n",
    "            random_state (int, optional): set random generator.  needed for unit testing. \n",
    "\n",
    "        Attributes:\n",
    "            base (estimator): Your general weak learner \n",
    "            n_learners (int): Number of weak learners in classifier.\n",
    "            alpha (ndarray): Coefficients on weak learners. \n",
    "            learners (list): List of weak learner instances. \n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        self.n_learners = n_learners \n",
    "        self.base = base\n",
    "        self.alpha = np.zeros(self.n_learners)\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier on data. Sets alphas and learners. \n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "\n",
    "        # Note: You can create and train a new instantiation \n",
    "        # of your sklearn decision tree as follows \n",
    "\n",
    "        m=len(y_train)\n",
    "        weights=[1/m]*m\n",
    "        w = np.asarray(weights)\n",
    "        h = clone(self.base)\n",
    "        train_o=[np.zeros(len(X_train))]\n",
    "        \n",
    "        for k in range(self.n_learners):\n",
    "            #print(\"In fit\",k)\n",
    "            h.fit(X_train, y_train, sample_weight=w)\n",
    "            train_o=h.predict(X_train)\n",
    "            \n",
    "            e=[int(x) for x in (train_o!=y_train)]\n",
    "            \n",
    "            err_k=np.dot(w,e) / sum(w)\n",
    "            \n",
    "            alpha_k=0.5*np.log((1-err_k)/(err_k))\n",
    "            \n",
    "            mul=train_o*y_train\n",
    "            \n",
    "            w=np.multiply(w,np.exp(-1*alpha_k*mul))\n",
    "            \n",
    "            self.alpha[k]=alpha_k\n",
    "            self.learners.append(train_o)\n",
    "            \n",
    "            #\"\"\"\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Adaboost prediction for new data X.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns: \n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO \n",
    "        \n",
    "       \n",
    "        pred=np.zeros(len(X))\n",
    "        #pred_f=[]\n",
    "        for k in range(self.n_learners):\n",
    "            for x2 in self.learners[k]:\n",
    "                y=x2 * self.alpha[k]\n",
    "                pred=[sum(pred,y)]\n",
    "                \n",
    "            #pred = [sum(x2) for x2 in zip(pred, [x2 * self.alpha[k] for x2 in self.learners[k]])]\n",
    "            \n",
    "        pred_f=np.sign(pred)\n",
    "        return pred_f\n",
    "        \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy of classifier.  \n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            Prediction accuracy (between 0.0 and 1.0).\n",
    "        \"\"\"\n",
    "        # TODO \n",
    "        \n",
    "        pred=np.zeros(len(X))\n",
    "        for k in range(self.n_learners):\n",
    "            pred = [sum(x3) for x3 in zip(pred, [x3 * self.alpha[k] for x3 in self.learners[k]])]\n",
    "        \n",
    "        pred_z=np.sign(pred)\n",
    "        print_z=[]\n",
    "        for i in pred_z:\n",
    "            print_z.append(int(i))\n",
    "        f=0\n",
    "        for i, j in zip(print_z,y):\n",
    "            if i==j:\n",
    "                f=f+1       \n",
    "        acc=f/len(y)\n",
    "        return acc\n",
    "        \n",
    "    \n",
    "    def staged_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the ensemble score after each iteration of boosting \n",
    "        for monitoring purposes, such as to determine the score on a \n",
    "        test set after each boost.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            scores (ndarary): [n_learners] ndarray of scores \n",
    "        \"\"\"\n",
    "\n",
    "        # TODO \n",
    "        f=np.zeros((self.n_learners))\n",
    "        #print(\"The len of\",(self.n_learners))\n",
    "        pred=np.zeros(len(X))\n",
    "        acc=[]\n",
    "        pred=np.zeros(len(X))\n",
    "        for k in range(self.n_learners):\n",
    "            pred = [sum(x2) for x2 in zip(pred, [x2 * self.alpha[k] for x2 in self.learners[k]])]\n",
    "            \n",
    "            pred_z=np.sign(pred)\n",
    "            \n",
    "            print_z=[]\n",
    "            for i in pred_z:\n",
    "                print_z.append(int(i))\n",
    "                \n",
    "            for i, j in zip(print_z,y):\n",
    "                if i==j:\n",
    "                    f[k]+=1\n",
    "        acc=f/len(y)\n",
    "        return acc\n",
    "        \n",
    "        \n",
    "    \n",
    "    def staged_margin(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes the staged margin after each iteration of boosting \n",
    "        for a single training example x and true label y\n",
    "        \n",
    "        Args:\n",
    "            x (ndarray): [n_features] ndarray of data \n",
    "            y (integer): an integer {-1,1} representing the true label of x \n",
    "            \n",
    "        Returns: \n",
    "            margins (ndarary): [n_learners] ndarray of margins \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "        alpha_normalize=self.alpha/sum(self.alpha)\n",
    "        margins=np.zeros(self.n_learners)\n",
    "            \n",
    "        for k in range(self.n_learners):\n",
    "            margins = [sum(x3) for x3 in zip(margins, [x3 * alpha_normalize[k] for x3 in self.learners[k]])]\n",
    "       \n",
    "        return margins\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the model we attempt to learn in AdaBoost is given by \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $h_k({\\bf x})$ is the $k^\\textrm{th}$ weak learner and $\\alpha_k$ is it's associated ensemble coefficient.  \n",
    "\n",
    "**Part A**: Implement the `fit` method to learn the sequence of weak learners $\\left\\{h_k({\\bf x})\\right\\}_{k=1}^K$ and corresponding coefficients $\\left\\{ \\alpha_k\\right\\}_{k=1}^K$. Note that you may use sklearn's implementation of DecisionTreeClassifier as your weak learner which allows you to pass as an optional parameter the weights associated with each training example.  An example of instantiating and training a single learner is given in the comments of the `fit` method.  \n",
    "\n",
    "When you think you're done, run the following unit tests which corresponds to the AdaBoost example given in the lecture slides. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:25.957948Z",
     "start_time": "2018-04-02T09:58:25.948471Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_alphas (__main__.TestBoost) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your `fit` method is working properly, implement the `predict` method to make predictions for unseen examples stored in a data matrix ${\\bf X}$.  \n",
    "\n",
    "**Note**: Remember that AdaBoost assumes that your predictions are of the form $y \\in \\{-1, 1\\}$. \n",
    "\n",
    "When you think you're done, run the following unit tests which corresponds to the AdaBoost example given in the lecture slides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:29.697855Z",
     "start_time": "2018-04-02T09:58:29.689876Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_prediction (__main__.TestBoost) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Next, implement the `score` method which takes in a matrix of examples ${\\bf X}$ and their associated true labels ${\\bf y}$, makes predictions, and returns the classification accuracy.   \n",
    "\n",
    "When you think you're done, run the following unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:32.327702Z",
     "start_time": "2018-04-02T09:58:32.320424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_score (__main__.TestBoost) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Finally, implement the `staged_score` method to return an array of prediction accuracies after each iteration of the AdaBoost algorithm.  That is, the staged score array ${\\bf s}$ is defined such that ${\\bf s}_\\ell$ is the prediction accuracy using only the first $\\ell$ weak learners.  This function is primarily used as a diagnostic tool for analyzing the performance of your classifier during the training process.  \n",
    "\n",
    "**Note**: This method can be implemented in a very efficient or very **in**efficient matter.  Be sure to think about this a bit before diving in. \n",
    "\n",
    "\n",
    "When you think you're done, run the following unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:36.952096Z",
     "start_time": "2018-04-02T09:58:36.944919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_staged_score (__main__.TestBoost) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 3: AdaBoost for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "Use your AdaBoost code with Sklearn's DecisionTreeClassifier as the base learner to distinguish $3$'s from $8$'s. \n",
    "Run $n=500$ boosting iterations with trees of depths 1, 2, and 3 (go deeper if you like) as the weak learner. For each weak learner, plot the training and validation error per boosting iteration (on the same set of axes). Compare and contrast the different weak learners. Which works the best? Do you see signs of overfitting? Do any of classifiers achieve nearly 100% accuracy on the training data? What happens to the accuracy on the validation data on further iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fit 0\n",
      "In fit 1\n",
      "In fit 2\n",
      "In fit 3\n",
      "In fit 4\n",
      "In fit 5\n",
      "In fit 6\n",
      "In fit 7\n",
      "In fit 8\n",
      "In fit 9\n",
      "In fit 10\n",
      "In fit 11\n",
      "In fit 12\n",
      "In fit 13\n",
      "In fit 14\n",
      "In fit 15\n",
      "In fit 16\n",
      "In fit 17\n",
      "In fit 18\n",
      "In fit 19\n",
      "In fit 20\n",
      "In fit 21\n",
      "In fit 22\n",
      "In fit 23\n",
      "In fit 24\n",
      "In fit 25\n",
      "In fit 26\n",
      "In fit 27\n",
      "In fit 28\n",
      "In fit 29\n",
      "In fit 30\n",
      "In fit 31\n",
      "In fit 32\n",
      "In fit 33\n",
      "In fit 34\n",
      "In fit 35\n",
      "In fit 36\n",
      "In fit 37\n",
      "In fit 38\n",
      "In fit 39\n",
      "In fit 40\n",
      "In fit 41\n",
      "In fit 42\n",
      "In fit 43\n",
      "In fit 44\n",
      "In fit 45\n",
      "In fit 46\n",
      "In fit 47\n",
      "In fit 48\n",
      "In fit 49\n",
      "In fit 50\n",
      "In fit 51\n",
      "In fit 52\n",
      "In fit 53\n",
      "In fit 54\n",
      "In fit 55\n",
      "In fit 56\n",
      "In fit 57\n",
      "In fit 58\n",
      "In fit 59\n",
      "In fit 60\n",
      "In fit 61\n",
      "In fit 62\n",
      "In fit 63\n",
      "In fit 64\n",
      "In fit 65\n",
      "In fit 66\n",
      "In fit 67\n",
      "In fit 68\n",
      "In fit 69\n",
      "In fit 70\n",
      "In fit 71\n",
      "In fit 72\n",
      "In fit 73\n",
      "In fit 74\n",
      "In fit 75\n",
      "In fit 76\n",
      "In fit 77\n",
      "In fit 78\n",
      "In fit 79\n",
      "In fit 80\n",
      "In fit 81\n",
      "In fit 82\n",
      "In fit 83\n",
      "In fit 84\n",
      "In fit 85\n",
      "In fit 86\n",
      "In fit 87\n",
      "In fit 88\n",
      "In fit 89\n",
      "In fit 90\n",
      "In fit 91\n",
      "In fit 92\n",
      "In fit 93\n",
      "In fit 94\n",
      "In fit 95\n",
      "In fit 96\n",
      "In fit 97\n",
      "In fit 98\n",
      "In fit 99\n",
      "In fit 100\n",
      "In fit 101\n",
      "In fit 102\n",
      "In fit 103\n",
      "In fit 104\n",
      "In fit 105\n",
      "In fit 106\n",
      "In fit 107\n",
      "In fit 108\n",
      "In fit 109\n",
      "In fit 110\n",
      "In fit 111\n",
      "In fit 112\n",
      "In fit 113\n",
      "In fit 114\n",
      "In fit 115\n",
      "In fit 116\n",
      "In fit 117\n",
      "In fit 118\n",
      "In fit 119\n",
      "In fit 120\n",
      "In fit 121\n",
      "In fit 122\n",
      "In fit 123\n",
      "In fit 124\n",
      "In fit 125\n",
      "In fit 126\n",
      "In fit 127\n",
      "In fit 128\n",
      "In fit 129\n",
      "In fit 130\n",
      "In fit 131\n",
      "In fit 132\n",
      "In fit 133\n",
      "In fit 134\n",
      "In fit 135\n",
      "In fit 136\n",
      "In fit 137\n",
      "In fit 138\n",
      "In fit 139\n",
      "In fit 140\n",
      "In fit 141\n",
      "In fit 142\n",
      "In fit 143\n",
      "In fit 144\n",
      "In fit 145\n",
      "In fit 146\n",
      "In fit 147\n",
      "In fit 148\n",
      "In fit 149\n",
      "In fit 150\n",
      "In fit 151\n",
      "In fit 152\n",
      "In fit 153\n",
      "In fit 154\n",
      "In fit 155\n",
      "In fit 156\n",
      "In fit 157\n",
      "In fit 158\n",
      "In fit 159\n",
      "In fit 160\n",
      "In fit 161\n",
      "In fit 162\n",
      "In fit 163\n",
      "In fit 164\n",
      "In fit 165\n",
      "In fit 166\n",
      "In fit 167\n",
      "In fit 168\n",
      "In fit 169\n",
      "In fit 170\n",
      "In fit 171\n",
      "In fit 172\n",
      "In fit 173\n",
      "In fit 174\n",
      "In fit 175\n",
      "In fit 176\n",
      "In fit 177\n",
      "In fit 178\n",
      "In fit 179\n",
      "In fit 180\n",
      "In fit 181\n",
      "In fit 182\n",
      "In fit 183\n",
      "In fit 184\n",
      "In fit 185\n",
      "In fit 186\n",
      "In fit 187\n",
      "In fit 188\n",
      "In fit 189\n",
      "In fit 190\n",
      "In fit 191\n",
      "In fit 192\n",
      "In fit 193\n",
      "In fit 194\n",
      "In fit 195\n",
      "In fit 196\n",
      "In fit 197\n",
      "In fit 198\n",
      "In fit 199\n",
      "D\n",
      "The len of 200\n",
      "In staged score 0\n",
      "In staged score 1\n",
      "In staged score 2\n",
      "In staged score 3\n",
      "In staged score 4\n",
      "In staged score 5\n",
      "In staged score 6\n",
      "In staged score 7\n",
      "In staged score 8\n",
      "In staged score 9\n",
      "In staged score 10\n",
      "In staged score 11\n",
      "In staged score 12\n",
      "In staged score 13\n",
      "In staged score 14\n",
      "In staged score 15\n",
      "In staged score 16\n",
      "In staged score 17\n",
      "In staged score 18\n",
      "In staged score 19\n",
      "In staged score 20\n",
      "In staged score 21\n",
      "In staged score 22\n",
      "In staged score 23\n",
      "In staged score 24\n",
      "In staged score 25\n",
      "In staged score 26\n",
      "In staged score 27\n",
      "In staged score 28\n",
      "In staged score 29\n",
      "In staged score 30\n",
      "In staged score 31\n",
      "In staged score 32\n",
      "In staged score 33\n",
      "In staged score 34\n",
      "In staged score 35\n",
      "In staged score 36\n",
      "In staged score 37\n",
      "In staged score 38\n",
      "In staged score 39\n",
      "In staged score 40\n",
      "In staged score 41\n",
      "In staged score 42\n",
      "In staged score 43\n",
      "In staged score 44\n",
      "In staged score 45\n",
      "In staged score 46\n",
      "In staged score 47\n",
      "In staged score 48\n",
      "In staged score 49\n",
      "In staged score 50\n",
      "In staged score 51\n",
      "In staged score 52\n",
      "In staged score 53\n",
      "In staged score 54\n",
      "In staged score 55\n",
      "In staged score 56\n",
      "In staged score 57\n",
      "In staged score 58\n",
      "In staged score 59\n",
      "In staged score 60\n",
      "In staged score 61\n",
      "In staged score 62\n",
      "In staged score 63\n",
      "In staged score 64\n",
      "In staged score 65\n",
      "In staged score 66\n",
      "In staged score 67\n",
      "In staged score 68\n",
      "In staged score 69\n",
      "In staged score 70\n",
      "In staged score 71\n",
      "In staged score 72\n",
      "In staged score 73\n",
      "In staged score 74\n",
      "In staged score 75\n",
      "In staged score 76\n",
      "In staged score 77\n",
      "In staged score 78\n",
      "In staged score 79\n",
      "In staged score 80\n",
      "In staged score 81\n",
      "In staged score 82\n",
      "In staged score 83\n",
      "In staged score 84\n",
      "In staged score 85\n",
      "In staged score 86\n",
      "In staged score 87\n",
      "In staged score 88\n",
      "In staged score 89\n",
      "In staged score 90\n",
      "In staged score 91\n",
      "In staged score 92\n",
      "In staged score 93\n",
      "In staged score 94\n",
      "In staged score 95\n",
      "In staged score 96\n",
      "In staged score 97\n",
      "In staged score 98\n",
      "In staged score 99\n",
      "In staged score 100\n",
      "In staged score 101\n",
      "In staged score 102\n",
      "In staged score 103\n",
      "In staged score 104\n",
      "In staged score 105\n",
      "In staged score 106\n",
      "In staged score 107\n",
      "In staged score 108\n",
      "In staged score 109\n",
      "In staged score 110\n",
      "In staged score 111\n",
      "In staged score 112\n",
      "In staged score 113\n",
      "In staged score 114\n",
      "In staged score 115\n",
      "In staged score 116\n",
      "In staged score 117\n",
      "In staged score 118\n",
      "In staged score 119\n",
      "In staged score 120\n",
      "In staged score 121\n",
      "In staged score 122\n",
      "In staged score 123\n",
      "In staged score 124\n",
      "In staged score 125\n",
      "In staged score 126\n",
      "In staged score 127\n",
      "In staged score 128\n",
      "In staged score 129\n",
      "In staged score 130\n",
      "In staged score 131\n",
      "In staged score 132\n",
      "In staged score 133\n",
      "In staged score 134\n",
      "In staged score 135\n",
      "In staged score 136\n",
      "In staged score 137\n",
      "In staged score 138\n",
      "In staged score 139\n",
      "In staged score 140\n",
      "In staged score 141\n",
      "In staged score 142\n",
      "In staged score 143\n",
      "In staged score 144\n",
      "In staged score 145\n",
      "In staged score 146\n",
      "In staged score 147\n",
      "In staged score 148\n",
      "In staged score 149\n",
      "In staged score 150\n",
      "In staged score 151\n",
      "In staged score 152\n",
      "In staged score 153\n",
      "In staged score 154\n",
      "In staged score 155\n",
      "In staged score 156\n",
      "In staged score 157\n",
      "In staged score 158\n",
      "In staged score 159\n",
      "In staged score 160\n",
      "In staged score 161\n",
      "In staged score 162\n",
      "In staged score 163\n",
      "In staged score 164\n",
      "In staged score 165\n",
      "In staged score 166\n",
      "In staged score 167\n",
      "In staged score 168\n",
      "In staged score 169\n",
      "In staged score 170\n",
      "In staged score 171\n",
      "In staged score 172\n",
      "In staged score 173\n",
      "In staged score 174\n",
      "In staged score 175\n",
      "In staged score 176\n",
      "In staged score 177\n",
      "In staged score 178\n",
      "In staged score 179\n",
      "In staged score 180\n",
      "In staged score 181\n",
      "In staged score 182\n",
      "In staged score 183\n",
      "In staged score 184\n",
      "In staged score 185\n",
      "In staged score 186\n",
      "In staged score 187\n",
      "In staged score 188\n",
      "In staged score 189\n",
      "In staged score 190\n",
      "In staged score 191\n",
      "In staged score 192\n",
      "In staged score 193\n",
      "In staged score 194\n",
      "In staged score 195\n",
      "In staged score 196\n",
      "In staged score 197\n",
      "In staged score 198\n",
      "In staged score 199\n",
      "[0.8865 0.8865 0.8865 0.9135 0.899  0.9155 0.9075 0.9225 0.923  0.926\n",
      " 0.9255 0.9285 0.9245 0.9305 0.9285 0.9355 0.9395 0.939  0.9395 0.9415\n",
      " 0.943  0.943  0.9465 0.9485 0.9475 0.949  0.95   0.953  0.951  0.9525\n",
      " 0.9535 0.9535 0.9535 0.9565 0.9575 0.9585 0.9575 0.9615 0.9615 0.9625\n",
      " 0.963  0.9655 0.9645 0.965  0.965  0.966  0.966  0.966  0.9655 0.966\n",
      " 0.9655 0.963  0.966  0.966  0.965  0.968  0.9665 0.967  0.9665 0.97\n",
      " 0.969  0.9715 0.9675 0.971  0.97   0.973  0.9705 0.9715 0.972  0.972\n",
      " 0.976  0.975  0.9775 0.977  0.977  0.978  0.978  0.977  0.978  0.9785\n",
      " 0.9785 0.979  0.977  0.98   0.9795 0.9805 0.9805 0.9825 0.981  0.9825\n",
      " 0.9835 0.9825 0.984  0.984  0.9855 0.9835 0.9845 0.985  0.985  0.983\n",
      " 0.985  0.9855 0.986  0.9855 0.9865 0.986  0.988  0.9865 0.988  0.9865\n",
      " 0.9885 0.987  0.9875 0.988  0.9875 0.988  0.9875 0.988  0.9875 0.989\n",
      " 0.988  0.989  0.9885 0.988  0.9905 0.991  0.9885 0.9905 0.988  0.9915\n",
      " 0.9875 0.9915 0.9915 0.9935 0.992  0.9935 0.994  0.994  0.993  0.9935\n",
      " 0.993  0.994  0.9935 0.993  0.9945 0.9935 0.994  0.995  0.9945 0.9945\n",
      " 0.9945 0.995  0.9945 0.995  0.9955 0.9955 0.9955 0.9965 0.995  0.996\n",
      " 0.996  0.996  0.9945 0.996  0.9955 0.996  0.996  0.996  0.996  0.996\n",
      " 0.996  0.9965 0.9965 0.9965 0.9965 0.9975 0.998  0.9965 0.9965 0.9975\n",
      " 0.997  0.9975 0.997  0.997  0.997  0.9975 0.997  0.997  0.997  0.997\n",
      " 0.998  0.998  0.998  0.9975 0.998  0.998  0.9975 0.998  0.9985 0.998 ]\n",
      "o\n",
      "The len of 200\n",
      "In staged score 0\n",
      "In staged score 1\n",
      "In staged score 2\n",
      "In staged score 3\n",
      "In staged score 4\n",
      "In staged score 5\n",
      "In staged score 6\n",
      "In staged score 7\n",
      "In staged score 8\n",
      "In staged score 9\n",
      "In staged score 10\n",
      "In staged score 11\n",
      "In staged score 12\n",
      "In staged score 13\n",
      "In staged score 14\n",
      "In staged score 15\n",
      "In staged score 16\n",
      "In staged score 17\n",
      "In staged score 18\n",
      "In staged score 19\n",
      "In staged score 20\n",
      "In staged score 21\n",
      "In staged score 22\n",
      "In staged score 23\n",
      "In staged score 24\n",
      "In staged score 25\n",
      "In staged score 26\n",
      "In staged score 27\n",
      "In staged score 28\n",
      "In staged score 29\n",
      "In staged score 30\n",
      "In staged score 31\n",
      "In staged score 32\n",
      "In staged score 33\n",
      "In staged score 34\n",
      "In staged score 35\n",
      "In staged score 36\n",
      "In staged score 37\n",
      "In staged score 38\n",
      "In staged score 39\n",
      "In staged score 40\n",
      "In staged score 41\n",
      "In staged score 42\n",
      "In staged score 43\n",
      "In staged score 44\n",
      "In staged score 45\n",
      "In staged score 46\n",
      "In staged score 47\n",
      "In staged score 48\n",
      "In staged score 49\n",
      "In staged score 50\n",
      "In staged score 51\n",
      "In staged score 52\n",
      "In staged score 53\n",
      "In staged score 54\n",
      "In staged score 55\n",
      "In staged score 56\n",
      "In staged score 57\n",
      "In staged score 58\n",
      "In staged score 59\n",
      "In staged score 60\n",
      "In staged score 61\n",
      "In staged score 62\n",
      "In staged score 63\n",
      "In staged score 64\n",
      "In staged score 65\n",
      "In staged score 66\n",
      "In staged score 67\n",
      "In staged score 68\n",
      "In staged score 69\n",
      "In staged score 70\n",
      "In staged score 71\n",
      "In staged score 72\n",
      "In staged score 73\n",
      "In staged score 74\n",
      "In staged score 75\n",
      "In staged score 76\n",
      "In staged score 77\n",
      "In staged score 78\n",
      "In staged score 79\n",
      "In staged score 80\n",
      "In staged score 81\n",
      "In staged score 82\n",
      "In staged score 83\n",
      "In staged score 84\n",
      "In staged score 85\n",
      "In staged score 86\n",
      "In staged score 87\n",
      "In staged score 88\n",
      "In staged score 89\n",
      "In staged score 90\n",
      "In staged score 91\n",
      "In staged score 92\n",
      "In staged score 93\n",
      "In staged score 94\n",
      "In staged score 95\n",
      "In staged score 96\n",
      "In staged score 97\n",
      "In staged score 98\n",
      "In staged score 99\n",
      "In staged score 100\n",
      "In staged score 101\n",
      "In staged score 102\n",
      "In staged score 103\n",
      "In staged score 104\n",
      "In staged score 105\n",
      "In staged score 106\n",
      "In staged score 107\n",
      "In staged score 108\n",
      "In staged score 109\n",
      "In staged score 110\n",
      "In staged score 111\n",
      "In staged score 112\n",
      "In staged score 113\n",
      "In staged score 114\n",
      "In staged score 115\n",
      "In staged score 116\n",
      "In staged score 117\n",
      "In staged score 118\n",
      "In staged score 119\n",
      "In staged score 120\n",
      "In staged score 121\n",
      "In staged score 122\n",
      "In staged score 123\n",
      "In staged score 124\n",
      "In staged score 125\n",
      "In staged score 126\n",
      "In staged score 127\n",
      "In staged score 128\n",
      "In staged score 129\n",
      "In staged score 130\n",
      "In staged score 131\n",
      "In staged score 132\n",
      "In staged score 133\n",
      "In staged score 134\n",
      "In staged score 135\n",
      "In staged score 136\n",
      "In staged score 137\n",
      "In staged score 138\n",
      "In staged score 139\n",
      "In staged score 140\n",
      "In staged score 141\n",
      "In staged score 142\n",
      "In staged score 143\n",
      "In staged score 144\n",
      "In staged score 145\n",
      "In staged score 146\n",
      "In staged score 147\n",
      "In staged score 148\n",
      "In staged score 149\n",
      "In staged score 150\n",
      "In staged score 151\n",
      "In staged score 152\n",
      "In staged score 153\n",
      "In staged score 154\n",
      "In staged score 155\n",
      "In staged score 156\n",
      "In staged score 157\n",
      "In staged score 158\n",
      "In staged score 159\n",
      "In staged score 160\n",
      "In staged score 161\n",
      "In staged score 162\n",
      "In staged score 163\n",
      "In staged score 164\n",
      "In staged score 165\n",
      "In staged score 166\n",
      "In staged score 167\n",
      "In staged score 168\n",
      "In staged score 169\n",
      "In staged score 170\n",
      "In staged score 171\n",
      "In staged score 172\n",
      "In staged score 173\n",
      "In staged score 174\n",
      "In staged score 175\n",
      "In staged score 176\n",
      "In staged score 177\n",
      "In staged score 178\n",
      "In staged score 179\n",
      "In staged score 180\n",
      "In staged score 181\n",
      "In staged score 182\n",
      "In staged score 183\n",
      "In staged score 184\n",
      "In staged score 185\n",
      "In staged score 186\n",
      "In staged score 187\n",
      "In staged score 188\n",
      "In staged score 189\n",
      "In staged score 190\n",
      "In staged score 191\n",
      "In staged score 192\n",
      "In staged score 193\n",
      "In staged score 194\n",
      "In staged score 195\n",
      "In staged score 196\n",
      "In staged score 197\n",
      "In staged score 198\n",
      "In staged score 199\n",
      "[0.494 0.494 0.494 0.476 0.494 0.482 0.486 0.474 0.476 0.474 0.474 0.476\n",
      " 0.482 0.476 0.486 0.48  0.486 0.484 0.48  0.482 0.478 0.474 0.476 0.484\n",
      " 0.482 0.48  0.48  0.482 0.48  0.478 0.476 0.482 0.484 0.486 0.484 0.48\n",
      " 0.478 0.48  0.476 0.48  0.48  0.48  0.48  0.484 0.482 0.484 0.482 0.484\n",
      " 0.482 0.484 0.484 0.48  0.484 0.482 0.482 0.488 0.482 0.484 0.486 0.484\n",
      " 0.484 0.486 0.486 0.488 0.484 0.49  0.486 0.49  0.486 0.488 0.484 0.49\n",
      " 0.488 0.494 0.49  0.494 0.49  0.494 0.49  0.494 0.49  0.494 0.494 0.494\n",
      " 0.494 0.492 0.49  0.49  0.49  0.49  0.492 0.488 0.494 0.492 0.494 0.492\n",
      " 0.49  0.494 0.492 0.488 0.49  0.492 0.492 0.492 0.492 0.49  0.492 0.492\n",
      " 0.492 0.492 0.492 0.492 0.492 0.492 0.492 0.492 0.492 0.492 0.492 0.492\n",
      " 0.492 0.492 0.492 0.492 0.494 0.494 0.494 0.494 0.494 0.492 0.494 0.494\n",
      " 0.492 0.492 0.492 0.494 0.492 0.492 0.492 0.492 0.492 0.492 0.494 0.492\n",
      " 0.492 0.492 0.492 0.49  0.492 0.49  0.49  0.49  0.49  0.49  0.49  0.49\n",
      " 0.49  0.492 0.49  0.49  0.49  0.49  0.492 0.49  0.49  0.492 0.492 0.492\n",
      " 0.492 0.492 0.492 0.492 0.492 0.49  0.492 0.492 0.49  0.49  0.49  0.49\n",
      " 0.49  0.49  0.49  0.49  0.49  0.49  0.49  0.49  0.49  0.49  0.49  0.49\n",
      " 0.49  0.49  0.49  0.492 0.49  0.492 0.492 0.492]\n",
      "N\n",
      "E\n",
      "SS\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxFJoP8PvvQQnRfQnf4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "itr=np.arange(200)\n",
    "train_err=[]\n",
    "test_err=[]\n",
    "ad=AdaBoost(n_learners=200,base=DecisionTreeClassifier(max_depth=1),random_state=1234)\n",
    "ad.fit(data.X_train,data.y_train)\n",
    "print(\"D\")\n",
    "#tr_err,te_err=ad.error_train()\n",
    "tr_err=ad.staged_score(data.X_train,data.y_train)\n",
    "print(\"o\")\n",
    "#print(tr_err[0])\n",
    "te_err=ad.staged_score(data.X_valid,data.y_valid)\n",
    "print(\"N\")\n",
    "#train_err.append(tr_err)\n",
    "#test_err.append(te_err)\n",
    "#itr.append(it)\n",
    "\n",
    "plt.plot(itr,tr_err,'b')\n",
    "print(\"E\")\n",
    "plt.plot(itr,te_err,'r')\n",
    "print(\"SS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_er3=[]\n",
    "te_er3=[]\n",
    "itr=range(0,600,100)\n",
    "for it in itr:\n",
    "    ad3=AdaBoostADV(it,DecisionTreeClassifier(max_depth=3),1234)\n",
    "    x3,y3=ad3.error()\n",
    "    tr_er3.append(x3)\n",
    "    te_er3.append(y3)\n",
    "\n",
    "it3=[0,100,200,300,400,500]    \n",
    "#plt.plot(it3,tr_er3,'r',it3,te_er3,'b')\n",
    "plt.plot(it3,tr_er3,'r')\n",
    "plt.plot(it3,te_er3,'b')\n",
    "plt.legend(['train_error','test_error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 4: AdaBoost as a Margin-Maximizing Model \n",
    "***\n",
    "\n",
    "Despite the fact that we're making our model more complex with the addition of each weak learner, AdaBoost does not typically overfit the training data. The reason for this is that the model becomes more _confident_ with each boosting iteration. This _confidence_ can be interpreted mathematically as a margin. Recall that after $K$ iterations the algorithm terminates with the classifier \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "Similarly, we can define the intermediate classifier $H_\\ell$ by \n",
    "\n",
    "$$\n",
    "H_\\ell({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^\\ell\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $\\ell \\leq K$. Note that in either case the model returns predictions of the form $y \\in \\{-1, 1\\}$ which does not give us any indication of the model's confidence in a prediction. Define the normalized coefficients $\\hat{\\alpha}_k$ as follows: \n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_k = \\dfrac{\\alpha_k}{\\sum_{t=1}^K \\alpha_k}\n",
    "$$\n",
    "\n",
    "Define the margin of a training example ${\\bf x}$ after $\\ell$ iterations as the sum of the normalized coefficients of weak learners that vote correctly minus the sum of the normalized coefficients of the weak learners that vote incorrectly: \n",
    "\n",
    "$$\n",
    "\\textrm{margin}_\\ell ({\\bf x}) = \\sum_{k=1:~h_k({\\bf x}) = y}^\\ell \\hat{\\alpha}_k - \\sum_{k=1:~h_k({\\bf x}) \\neq y}^\\ell \\hat{\\alpha}_k \n",
    "$$\n",
    "\n",
    "**Part A**: Briefly explain mathematically how $\\textrm{margin}_\\ell({\\bf x})$ can be interpreted as a margin.  **Hint**: You'll want to think back on what we meant by a _margin_ in our discussion of Support Vector Machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `staged_margin` method in the `AdaBoost` class above so that it computes the margin for a single training example ${\\bf x}$ after each boosting iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3782118834689394, -0.20725070451075467, 0.40006107202393093, 0.3006637545849843, 0.13943958719075117, -0.688806628866582, -0.5229749967194826, 0.3889612493437753, -0.37077037903677423, 0.550851728894537, -0.3706716411124431, -0.5780602257003224, 0.3833360595922294, 0.43520773542010915, 0.284417078549503, -0.17305500428177495, 0.12094340555313277, 0.5390907520708965, 0.3626645872167443, -0.4543287523641856]\n"
     ]
    }
   ],
   "source": [
    "ad=AdaBoost()\n",
    "ad.fit(data.X_train,data.y_train)\n",
    "margin_value=ad.staged_margin(data.X_train[0],data.y_train[0])\n",
    "print(margin_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Find four **training** examples from the MNIST that meet the following criteria: \n",
    "\n",
    "- one $3$ that AdaBoost can classify easily  \n",
    "- one $8$ that AdaBoost can classify easily  \n",
    "- one $3$ that AdaBoost has difficulty with \n",
    "- one $8$ that AdaBoost has difficulty with \n",
    "\n",
    "Use the `view_digit` function given below to display the four examples that you found. \n",
    "\n",
    "**Advice**: Since AdaBoost will likely classify **all** training examples correctly given enough boosting iterations, you might try fitting an AdaBoost classifier with just a handful of boosting iterations and use it to identify examples of each desired type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:59:01.257421Z",
     "start_time": "2018-04-02T09:59:01.068539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_digit(example, label=None):\n",
    "    if label: print(\"true label: {:d}\".format(label))\n",
    "    plt.imshow(example.reshape(21,21), cmap='gray');\n",
    "    \n",
    "view_digit(data.X_train[0,:], data.y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Using an AdaBoost classifier with at least $K=200$ depth-1 decision trees as the weak learners, plot the staged margin for each of the four examples that you found in **Part C** on the same set of axes. (Be sure to include a legend so we can tell which staged margin corresponds to which example).  Explain your results in terms of the margin of the classifier on each training examples.  More broadly, how the margin-maximizing property might allow AdaBoost to continue improving generalization even after the error on the training set reaches zero.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
